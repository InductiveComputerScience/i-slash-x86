; Integer
; Working registers -- rax, rcx, rdx
; Parameter: rdi
; Unused: rbx, rsi, r8-r15

; SSE
; Working registers -- xmm0, xmm1, xmm2
; Unused: xmm3-xmm15

; AVX
; Working registers -- ymm0, ymm1, ymm2, ymm3
; Unused: ymm4-ymm15

; -----------------------------------------------------------------------------
; Add

%macro Add.mmu8 3
  mov al, byte [rdi + %2]
  mov ah, byte [rdi + %3]
  add al, ah
  mov byte [rdi + %1], al
%endmacro

%macro Add.mmu16 3
  mov ax, word [rdi + %2]
  mov dx, word [rdi + %3]
  add ax, dx
  mov word [rdi + %1], ax
%endmacro

%macro Add.miu16 3
  mov ax, word [rdi + %2]
  mov dx, %3
  add ax, dx
  mov word [rdi + %1], ax
%endmacro

%macro Add.mmu32 3
  mov eax, dword [rdi + %2]
  mov edx, dword [rdi + %3]
  add eax, edx
  mov dword [rdi + %1], eax
%endmacro

%macro Add.miu64 3
  mov rax, qword [rdi + %2]
  mov rdx, %3
  add rax, rdx
  mov qword [rdi + %1], rax
%endmacro

%macro Add.mmu64 3
  mov rax, qword [rdi + %2]
  mov rdx, qword [rdi + %3]
  add rax, rdx
  mov qword [rdi + %1], rax
%endmacro

%macro Add.mis64 3
  mov rax, qword [rdi + %2]
  mov rdx, %3
  add rax, rdx
  mov qword [rdi + %1], rax
%endmacro

%macro Add.rrru64 3
  mov rax, %2
  add rax, %3
  mov %1, rax
%endmacro

%macro Add.miu8x32 3
  vmovdqu ymm0, yword [rdi + %2]
  mov rax, %3
  movq xmm2, rax
  vpbroadcastb ymm1, xmm2
  vpaddb ymm0, ymm0, ymm1
  vmovdqu yword [rdi + %1], ymm0
%endmacro

%macro Add.mms8 3
  mov al, byte [rdi + %2]
  mov ah, byte [rdi + %3]
  add al, ah
  mov byte [rdi + %1], al
%endmacro

%macro Add.mms16 3
  mov ax, word [rdi + %2]
  mov dx, word [rdi + %3]
  add ax, dx
  mov word [rdi + %1], ax
%endmacro

%macro Add.mms32 3
  mov eax, dword [rdi + %2]
  mov edx, dword [rdi + %3]
  add eax, edx
  mov dword [rdi + %1], eax
%endmacro

%macro Add.mms64 3
  mov rax, qword [rdi + %2]
  mov rdx, qword [rdi + %3]
  add rax, rdx
  mov qword [rdi + %1], rax
%endmacro

%macro Add.mmf32 3
  movss xmm0, dword [rdi + %2]
  addss xmm0, dword [rdi + %3]
  movss dword [rdi + %1], xmm0
%endmacro

%macro Add.mmf64 3
  movsd xmm0, qword [rdi + %2]
  addsd xmm0, qword [rdi + %3]
  movsd qword [rdi + %1], xmm0
%endmacro

; -----------------------------------------------------------------------------
; Sub

%macro Sub.mmu8 3
  mov al, byte [rdi + %2]
  mov ah, byte [rdi + %3]
  sub al, ah
  mov byte [rdi + %1], al
%endmacro

%macro Sub.mmu16 3
  mov ax, word [rdi + %2]
  mov dx, word [rdi + %3]
  sub ax, dx
  mov word [rdi + %1], ax
%endmacro

%macro Sub.mmu64 3
  mov rax, qword [rdi + %2]
  mov rdx, qword [rdi + %3]
  sub rax, rdx
  mov qword [rdi + %1], rax
%endmacro

%macro Sub.mms64 3
  mov rax, qword [rdi + %2]
  mov rdx, qword [rdi + %3]
  sub rax, rdx
  mov qword [rdi + %1], rax
%endmacro

%macro Sub.mis64 3
  mov rax, qword [rdi + %2]
  mov rdx, %3
  sub rax, rdx
  mov qword [rdi + %1], rax
%endmacro

%macro Sub.rrru64 3
  mov rax, %2
  sub rax, %3
  mov %1, rax
%endmacro

%macro Sub.mmf64 3
  movsd xmm0, [rdi + %2]
  subsd xmm0, [rdi + %3]
  movsd [rdi + %1], xmm0
%endmacro

%macro Sub.mmu8x32 3
  vmovdqu ymm0, yword [rdi + %2]
  vmovdqu ymm1, yword [rdi + %3]
  vpsubb ymm0, ymm0, ymm1
  vmovdqu yword [rdi + %1], ymm0
%endmacro

%macro Sub.rrru8x32 3
  vpsubb %1, %2, %3
%endmacro

%macro Sub.miu8x32 3
  vmovdqu ymm0, yword [rdi + %2]
  mov rax, %3
  movq xmm2, rax
  vpbroadcastb ymm1, xmm2
  vpsubb ymm0, ymm0, ymm1
  vmovdqu yword [rdi + %1], ymm0
%endmacro

; -----------------------------------------------------------------------------
; Mul

%macro Mul.mmu8 3
  mov al, byte [rdi + %2]
  mov ah, byte [rdi + %3]
  mul ah
  mov byte [rdi + %1], al
%endmacro

%macro Mul.mmu16 3
  mov ax, word [rdi + %2]
  mov dx, word [rdi + %3]
  mul dx
  mov word [rdi + %1], ax
%endmacro

%macro Mul.mmf64 3
  movsd xmm0, [rdi + %2]
  mulsd xmm0, [rdi + %3]
  movsd [rdi + %1], xmm0
%endmacro

%macro Mul.mmu64 3
  mov rax, qword [rdi + %2]
  mov rdx, qword [rdi + %3]
  mul rdx
  mov qword [rdi + %1], rax
%endmacro

%macro Mul.rrru64 3
  mov rax, %2
  mul %3
  mov %1, rax
%endmacro

%macro Mul.miu64 3
  mov rax, qword [rdi + %2]
  mov rdx, %3
  mul rdx
  mov qword [rdi + %1], rax
%endmacro

%macro Mul.imu64 3
  mov rax, %2
  mov rdx, qword [rdi + %3]
  mul rdx
  mov qword [rdi + %1], rax
%endmacro

%macro Mul.mis64 3
  mov rax, qword [rdi + %2]
  mov rdx, %3
  imul rax, rdx
  mov qword [rdi + %1], rax
%endmacro

%macro Mul.mms64 3
  mov rax, qword [rdi + %2]
  mov rdx, qword [rdi + %3]
  imul rax, rdx
  mov qword [rdi + %1], rax
%endmacro

%macro Mul.ims64 3
  mov rax, %2
  mov rdx, qword [rdi + %3]
  imul rax, rdx
  mov qword [rdi + %1], rax
%endmacro

%macro Mul.iis64 3
  mov rax, %2
  mov rdx, %3
  imul rax, rdx
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Div

%macro Div.mmu8 3
  mov al, byte [rdi + %2]
  mov ah, 0
  mov dl, byte [rdi + %3]
  mov dh, 0
  div dl
  mov byte [rdi + %1], al
%endmacro

%macro Div.mmu16 3
  mov ax, word [rdi + %2]
  mov dx, 0
  mov cx, word [rdi + %3]
  div cx
  mov word [rdi + %1], ax
%endmacro

%macro Div.mmu64 3
  mov rax, qword [rdi + %2]
  mov rdx, 0
  mov rcx, qword [rdi + %3]
  div rcx
  mov qword [rdi + %1], rax
%endmacro

%macro Div.rrru64 3
  mov rax, %2
  mov rdx, 0
  div %3
  mov %1, rax
%endmacro

%macro Div.miu64 3
  mov rax, qword [rdi + %2]
  mov rdx, 0
  mov rcx, %3
  div rcx
  mov qword [rdi + %1], rax
%endmacro

%macro Div.rmiu64 3
  mov rax, qword [rdi + %2]
  mov rdx, 0
  mov rcx, %3
  div rcx
  mov %1, rax
%endmacro

%macro Div.iiu64 3
  mov rax, %2
  mov rdx, 0
  mov rcx, %3
  div rcx
  mov qword [rdi + %1], rax
%endmacro

%macro Div.mmf64 3
  movsd xmm0, [rdi + %2]
  divsd xmm0, [rdi + %3]
  movsd [rdi + %1], xmm0
%endmacro

%macro Div.mis64 3
  mov rax, qword [rdi + %2]
  cqo
  mov rcx, %3
  idiv rcx
  mov qword [rdi + %1], rax
%endmacro

%macro Div.ims64 3
  mov rax, %2
  cqo
  mov rcx, qword [rdi + %3]
  idiv rcx
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; DivMod

%macro DivMod.mmu64 4
  mov rax, qword [rdi + %3]
  mov rdx, 0
  mov rcx, qword [rdi + %4]
  div rcx
  mov qword [rdi + %1], rax
  mov qword [rdi + %2], rdx
%endmacro

%macro DivMod.miu64 4
  mov rax, qword [rdi + %3]
  mov rdx, 0
  mov rcx, %4
  div rcx
  mov qword [rdi + %1], rax
  mov qword [rdi + %2], rdx
%endmacro

%macro DivMod.mms64 4
  mov rax, qword [rdi + %3]
  cqo
  mov rcx, qword [rdi + %4]
  idiv rcx
  mov qword [rdi + %1], rax
  mov qword [rdi + %2], rdx
%endmacro

; -----------------------------------------------------------------------------
; Mod

%macro Mod.mmu64 3
  mov rax, qword [rdi + %2]
  mov rdx, 0
  mov rcx, qword [rdi + %3]
  div rcx
  mov qword [rdi + %1], rdx
%endmacro

; -----------------------------------------------------------------------------
; MulDiv

%macro MulDiv.mmu64 4
  mov rax, qword [rdi + %2]
  mov rdx, qword [rdi + %3]
  mul rdx
  mov rcx, qword [rdi + %4]
  div rcx
  mov qword [rdi + %1], rax
%endmacro

%macro MulDiv.mms64 4
  mov rax, qword [rdi + %2]
  mov rdx, qword [rdi + %3]
  imul rdx
  mov rcx, qword [rdi + %4]
  idiv rcx
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Mov

%macro Mov.mb1 2
  mov al, byte [rdi + %2]
  and al, 1
  mov byte [rdi + %1], al
%endmacro

%macro Mov.ib1 2
  mov al, %2
  and al, 1
  mov byte [rdi + %1], al
%endmacro

%macro Mov.mu8 2
  mov al, byte [rdi + %2]
  mov byte [rdi + %1], al
%endmacro

%macro Mov.mu16 2
  mov ax, word [rdi + %2]
  mov word [rdi + %1], ax
%endmacro

%macro Mov.iu8 2
  mov al, %2
  mov byte [rdi + %1], al
%endmacro

%macro Mov.iu16 2
  mov ax, %2
  mov word [rdi + %1], ax
%endmacro

%macro Mov.iu32 2
  mov eax, %2
  mov dword [rdi + %1], eax
%endmacro

%macro Mov.iu64 2
  mov rax, %2
  mov qword [rdi + %1], rax
%endmacro

%macro Mov.riu64 2
  mov %1, %2
%endmacro

%macro Mov.mu64 2
  mov rax, qword [rdi + %2]
  mov qword [rdi + %1], rax
%endmacro

%macro Mov.ms64 2
  mov rax, qword [rdi + %2]
  mov qword [rdi + %1], rax
%endmacro

%macro Mov.mru64 2
  mov qword [rdi + %1], %2
%endmacro

%macro Mov.rmu64 2
  mov %1, qword [rdi + %2]
%endmacro

%macro Mov.iu8x32 2
  mov rax, %2
  movq xmm2, rax
  vpbroadcastb ymm0, xmm2
  vmovdqu yword [rdi + %1], ymm0
%endmacro

%macro Mov.riu8x32 2
  mov rax, %2
  movq xmm2, rax
  vpbroadcastb %1, xmm2
%endmacro

%macro Mov.is8 2
  mov al, %2
  mov byte [rdi + %1], al
%endmacro

%macro Mov.is16 2
  mov ax, %2
  mov word [rdi + %1], ax
%endmacro

%macro Mov.is32 2
  mov eax, %2
  mov dword [rdi + %1], eax
%endmacro

%macro Mov.is64 2
  mov rax, %2
  mov qword [rdi + %1], rax
%endmacro

%macro Mov.if32 2
  mov eax, __?float32?__(%2)
  mov dword [rdi + %1], eax
%endmacro

%macro Mov.if64 2
  mov rax, __?float64?__(%2)
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Loop

%macro Loop 1
  %1:
%endmacro

; -----------------------------------------------------------------------------
; Len

%macro Len 2
  mov rax, qword [rdi + %2]
  mov rax, qword [rax - 8]
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Lt

%macro Lt.mmu16 3
  mov ax, word [rdi + %2]
  mov dx, word [rdi + %3]
  cmp ax, dx
  setb al
  mov byte [rdi + %1], al
%endmacro

%macro Lt.mmu32 3
  mov eax, dword [rdi + %2]
  mov edx, dword [rdi + %3]
  cmp eax, edx
  setb al
  mov byte [rdi + %1], al
%endmacro

%macro Lt.mmu64 3
  mov rax, qword [rdi + %2]
  mov rdx, qword [rdi + %3]
  cmp rax, rdx
  setb al
  mov byte [rdi + %1], al
%endmacro

%macro Lt.rrru64 3
  cmp %2, %3
  setb %1
%endmacro

%macro Lt.iiu16 3
  mov ax, %2
  mov dx, %3
  cmp ax, dx
  setb al
  mov byte [rdi + %1], al
%endmacro

%macro Lt.iiu64 3
  mov rax, %2
  mov rdx, %3
  cmp rax, rdx
  setb al
  mov byte [rdi + %1], al
%endmacro

%macro Lt.mms16 3
  mov ax, word [rdi + %2]
  mov dx, word [rdi + %3]
  cmp ax, dx
  setl al
  mov byte [rdi + %1], al
%endmacro

%macro Lt.iis16 3
  mov ax, %2
  mov dx, %3
  cmp ax, dx
  setl al
  mov byte [rdi + %1], al
%endmacro

%macro Lt.miu8x32 3
  vmovdqu ymm0, yword [rdi + %2]
  mov rax, %3
  movq xmm2, rax
  vpbroadcastb ymm1, xmm2
  vpcmpgtb ymm0, ymm0, ymm1
  vmovdqu yword [rdi + %1], ymm0
%endmacro

%macro Lt.mif64 3
  movsd xmm0, [rdi + %2]
  mov rax, __?float64?__(%3)
  movq xmm1, rax
  cmpltsd xmm0, xmm1
  movq rax, xmm0
  cmp al, 255
  sete [rdi + %1]
%endmacro

%macro Lt.mis64 3
  mov rax, qword [rdi + %2]
  mov rdx, %3
  cmp rax, rdx
  setl al
  mov byte [rdi + %1], al
%endmacro

%macro Lt.mms64 3
  mov rax, qword [rdi + %2]
  mov rdx, qword [rdi + %3]
  cmp rax, rdx
  setl al
  mov byte [rdi + %1], al
%endmacro

; -----------------------------------------------------------------------------
; Lte

%macro Lte.mmu8x32 3
  vmovdqu ymm0, yword [rdi + %2]
  vmovdqu ymm1, yword [rdi + %3]
  vpcmpgtb ymm2, ymm0, ymm1
  vpcmpeqb ymm3, ymm3, ymm3
  vpxor ymm0, ymm2, ymm3
  vmovdqu yword [rdi + %1], ymm0
%endmacro

%macro Lte.rrru8x32 3
  vpcmpgtb ymm2, %2, %3
  vpcmpeqb ymm3, ymm3, ymm3
  vpxor %1, ymm2, ymm3
%endmacro

%macro Lte.miu8x32 3
  vmovdqu ymm0, yword [rdi + %2]
  mov rax, %3
  movq xmm2, rax
  vpbroadcastb ymm1, xmm2
  vpcmpgtb ymm2, ymm0, ymm1
  vpcmpeqb ymm3, ymm3, ymm3
  vpxor ymm0, ymm2, ymm3
  vmovdqu yword [rdi + %1], ymm0
%endmacro

; -----------------------------------------------------------------------------
; Gt

%macro Gt.miu8x32 3
  vmovdqu ymm0, yword [rdi + %2]
  mov rax, %3
  movq xmm2, rax
  vpbroadcastb ymm1, xmm2
  vpcmpgtb ymm0, ymm0, ymm1
  vmovdqu yword [rdi + %1], ymm0
%endmacro

%macro Gt.mif64 3
  movsd xmm0, [rdi + %2]
  mov rax, __?float64?__(%3)
  movq xmm1, rax
  cmpnlesd xmm0, xmm1
  movq rax, xmm0
  cmp al, 255
  sete [rdi + %1]
%endmacro

%macro Gt.mis64 3
  mov rax, qword [rdi + %2]
  mov rdx, %3
  cmp rax, rdx
  setg al
  mov byte [rdi + %1], al
%endmacro

; -----------------------------------------------------------------------------
; Gte

%macro Gte.mmu8x32 3
  vmovdqu ymm0, yword [rdi + %2]
  vmovdqu ymm1, yword [rdi + %3]
  vpcmpgtb ymm2, ymm0, ymm1
  vpcmpeqb ymm3, ymm0, ymm1
  vpor ymm0, ymm2, ymm3
  vmovdqu yword [rdi + %1], ymm0
%endmacro

%macro Gte.rmmu8x32 3
  vmovdqu ymm0, yword [rdi + %2]
  vmovdqu ymm1, yword [rdi + %3]
  vpcmpgtb ymm2, ymm0, ymm1
  vpcmpeqb ymm3, ymm0, ymm1
  vpor %1, ymm2, ymm3
%endmacro

%macro Gte.rrru8x32 3
  vpcmpgtb ymm2, %2, %3
  vpcmpeqb ymm3, %2, %3
  vpor %1, ymm2, ymm3
%endmacro

%macro Gte.miu8x32 3
  vmovdqu ymm0, yword [rdi + %2]
  mov rax, %3
  movq xmm2, rax
  vpbroadcastb ymm1, xmm2
  vpcmpgtb ymm2, ymm0, ymm1
  vpcmpeqb ymm3, ymm0, ymm1
  vpor ymm0, ymm2, ymm3
  vmovdqu yword [rdi + %1], ymm0
%endmacro

; -----------------------------------------------------------------------------
; Not

%macro Not.mb1 2
  mov al, byte [rdi + %2]
  not al
  and al, 1
  mov byte [rdi + %1], al 
%endmacro

%macro Not.mu8x32 2
  vmovdqu ymm0, yword [rdi + %2]
  vpcmpeqb ymm3, ymm3, ymm3
  vpxor ymm0, ymm0, ymm3
  vmovdqu yword [rdi + %1], ymm0
%endmacro

; -----------------------------------------------------------------------------
; And

%macro And.mmb1 3
  mov al, byte [rdi + %2]
  mov ah, byte [rdi + %3]
  and al, ah
  and al, 1
  mov byte [rdi + %1], al
%endmacro

%macro And.mmu8 3
  mov al, byte [rdi + %2]
  mov ah, byte [rdi + %3]
  and al, ah
  and al, 1
  mov byte [rdi + %1], al
%endmacro

%macro And.mmu8x32 3
  vmovdqu ymm0, yword [rdi + %2]
  vmovdqu ymm1, yword [rdi + %3]
  vpand ymm0, ymm0, ymm1
  vmovdqu yword [rdi + %1], ymm0
%endmacro

%macro And.rrru8x32 3
  vpand %1, %2, %3
%endmacro

%macro And.mmb8x32 3
  vmovdqu ymm0, yword [rdi + %2]
  vmovdqu ymm1, yword [rdi + %3]
  vpand ymm0, ymm0, ymm1
  vmovdqu yword [rdi + %1], ymm0
%endmacro

%macro And.rrrb8x32 3
  vpand %1, %2, %3
%endmacro

%macro And.miu8x32 3
  vmovdqu ymm0, yword [rdi + %2]
  vmovdqu ymm1, yword [rdi + %3]
  mov rax, %3
  movq xmm2, rax
  vpbroadcastb ymm1, xmm2
  vpand ymm0, ymm0, ymm1
  vmovdqu yword [rdi + %1], ymm0
%endmacro

; -----------------------------------------------------------------------------
; Andnot - y = !a & b

%macro Andnot.mmb64 3
  mov rax, qword [rdi + %2]
  mov rcx, qword [rdi + %3]
  andn rdx, rax, rcx
  mov qword [rdi + %1], rdx
%endmacro

; -----------------------------------------------------------------------------
; If

%macro If.mb1 2
  mov al, 0
  mov ah, byte [rdi + %1]
  and ah, 1
  cmp al, ah
  je %2
%endmacro

%macro If.rb1 2
  mov al, 0
  mov cl, %1
  and cl, 1
  cmp al, cl
  je %2
%endmacro

%macro If.mu8 2
  mov al, 0
  mov ah, byte [rdi + %1]
  cmp al, ah
  je %2
%endmacro

; -----------------------------------------------------------------------------
; Endloop

%macro EndLoop 2
  jmp %1
  %2:
%endmacro

; -----------------------------------------------------------------------------
; Endb

%macro Endb 1
  %1:
%endmacro

; -----------------------------------------------------------------------------
; Idr

%macro Idr.mmu16a 3
  mov rax, qword [rdi + %2]
  mov rdx, qword [rdi + %3]
  mov ax, word [rax + rdx * 2]
  mov word [rdi + %1], ax
%endmacro

%macro Idr.mmu32a 3
  mov rax, qword [rdi + %2]
  mov rdx, qword [rdi + %3]
  mov eax, dword [rax + rdx * 4]
  mov dword [rdi + %1], eax
%endmacro

%macro Idr.mmu64a 3
  mov rax, qword [rdi + %2]
  mov rdx, qword [rdi + %3]
  mov rax, qword [rax + rdx * 8]
  mov qword [rdi + %1], rax
%endmacro

%macro Idr.rrru64a 3
  mov %1, qword [%2 + %3 * 8]
%endmacro

%macro Idr.mmu8x32a 3
  mov rax, qword [rdi + %2]
  mov rdx, qword [rdi + %3]
  shl rdx, 5
  vmovdqu ymm0, yword [rax + rdx]
  vmovdqu yword [rdi + %1], ymm0
%endmacro

%macro Idr.rmmu8x32a 3
  mov rax, qword [rdi + %2]
  mov rdx, qword [rdi + %3]
  shl rdx, 5
  vmovdqu %1, yword [rax + rdx]
%endmacro

%macro Idr.rmru8x32a 3
  mov rax, qword [rdi + %2]
  mov rdx, %3
  shl rdx, 5
  vmovdqu %1, yword [rax + rdx]
%endmacro

%macro Idr.rrru8x32a 3
  mov rdx, %3
  shl rdx, 5
  vmovdqu %1, yword [%2 + rdx]
%endmacro

%macro Idr.mif64a 3
  mov rax, qword [rdi + %2]
  mov rdx, %3
  mov rax, qword [rax + rdx * 8]
  mov qword [rdi + %1], rax
%endmacro

%macro Idr.mista 3
  mov rax, qword [rdi + %2]
  mov rdx, %3
  mov rax, qword [rax + rdx * 8]
  mov qword [rdi + %1], rax
%endmacro



; -----------------------------------------------------------------------------
; Eq

%macro Eq.mmu8 3
  mov al, byte [rdi + %2]
  mov dl, byte [rdi + %3]
  cmp al, dl
  sete al
  mov byte [rdi + %1], al
%endmacro

%macro Eq.miu8 3
  mov al, byte [rdi + %2]
  mov dl, %3
  cmp al, dl
  sete al
  mov byte [rdi + %1], al
%endmacro

%macro Eq.mmu16 3
  mov ax, word [rdi + %2]
  mov dx, word [rdi + %3]
  cmp ax, dx
  sete al
  mov byte [rdi + %1], al
%endmacro

%macro Eq.miu16 3
  mov ax, word [rdi + %2]
  mov dx, %3
  cmp ax, dx
  sete al
  mov byte [rdi + %1], al
%endmacro

%macro Eq.mmu32 3
  mov eax, dword [rdi + %2]
  mov edx, dword [rdi + %3]
  cmp eax, edx
  sete al
  mov byte [rdi + %1], al
%endmacro

%macro Eq.miu32 3
  mov eax, dword [rdi + %2]
  mov edx, %3
  cmp eax, edx
  sete al
  mov byte [rdi + %1], al
%endmacro

%macro Eq.mmu64 3
  mov rax, qword [rdi + %2]
  mov rdx, qword [rdi + %3]
  cmp rax, rdx
  sete al
  mov byte [rdi + %1], al
%endmacro

%macro Eq.mms64 3
  mov rax, qword [rdi + %2]
  mov rdx, qword [rdi + %3]
  cmp rax, rdx
  sete al
  mov byte [rdi + %1], al
%endmacro

%macro Eq.miu64 3
  mov rax, qword [rdi + %2]
  mov rdx, %3
  cmp rax, rdx
  sete al
  mov byte [rdi + %1], al
%endmacro

%macro Eq.iiu16 3
  mov ax, %2
  mov dx, %3
  cmp ax, dx
  sete al
  mov byte [rdi + %1], al
%endmacro

%macro Eq.mib1 3
  mov al, byte [rdi + %2]
  mov dl, %3
  cmp al, dl
  sete al
  mov byte [rdi + %1], al
%endmacro

%macro Eq.mis8 3
  mov al, byte [rdi + %2]
  mov dl, %3
  cmp al, dl
  sete al
  mov byte [rdi + %1], al
%endmacro

%macro Eq.mis16 3
  mov ax, word [rdi + %2]
  mov dx, %3
  cmp ax, dx
  sete al
  mov byte [rdi + %1], al
%endmacro

%macro Eq.mis32 3
  mov eax, dword [rdi + %2]
  mov edx, %3
  cmp eax, edx
  sete al
  mov byte [rdi + %1], al
%endmacro

%macro Eq.mis64 3
  mov rax, qword [rdi + %2]
  mov rdx, %3
  cmp rax, rdx
  sete al
  mov byte [rdi + %1], al
%endmacro

%macro Eq.mif32 3
  movss xmm0, [rdi + %2]
  mov eax, __?float32?__(%3)
  movd xmm1, eax
  cmpeqss xmm0, xmm1
  movd eax, xmm0
  cmp al, 255
  sete [rdi + %1]
%endmacro

%macro Eq.mif64 3
  movsd xmm0, [rdi + %2]
  mov rax, __?float64?__(%3)
  movq xmm1, rax
  cmpeqsd xmm0, xmm1
  movq rax, xmm0
  cmp al, 255
  sete [rdi + %1]
%endmacro

; -----------------------------------------------------------------------------
; Acw

%macro Acw.mmu16 3
  mov ax, word [rdi + %3]
  mov rdx, qword [rdi + %1]
  mov word [rdx + %2], ax
%endmacro

%macro Acw.mmu64 3
  mov rax, qword [rdi + %3]
  mov rdx, qword [rdi + %1]
  mov qword [rdx + %2], rax
%endmacro

%macro Acw.mms64 3
  mov rax, qword [rdi + %3]
  mov rdx, qword [rdi + %1]
  mov qword [rdx + %2], rax
%endmacro

%macro Acw.miu64 3
  mov rax, %3
  mov rdx, qword [rdi + %1]
  mov qword [rdx + %2], rax
%endmacro

; -----------------------------------------------------------------------------
; Acr

%macro Acr.mmu64 3
  mov rax, [rdi + %2]
  mov rax, [rax + %3]
  mov [rdi + %1], rax
%endmacro

%macro Acr.mms64 3
  mov rax, [rdi + %2]
  mov rax, [rax + %3]
  mov [rdi + %1], rax
%endmacro

%macro Acr.mmb1 3
  mov rax, [rdi + %2]
  mov al, byte [rax + %3]
  mov [rdi + %1], al
%endmacro

%macro Acr.mmf64 3
  mov rax, [rdi + %2]
  mov rax, [rax + %3]
  mov [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Inc

%macro Inc.mu64 1
  inc qword [rdi + %1]
%endmacro

%macro Inc.ms64 1
  inc qword [rdi + %1]
%endmacro

%macro Inc.ru64 1
  inc %1
%endmacro

; -----------------------------------------------------------------------------
; Dec

%macro Dec.mu64 1
  dec qword [rdi + %1]
%endmacro

%macro Dec.ms64 1
  dec qword [rdi + %1]
%endmacro

%macro Dec.ru64 1
  dec %1
%endmacro

; -----------------------------------------------------------------------------
; Shl

%macro Shl.mib32 3
  mov eax, dword [rdi + %2]
  shl eax, %3
  mov dword [rdi + %1], eax
%endmacro

; -----------------------------------------------------------------------------
; Shr

%macro Shr.miu32 3
  mov eax, dword [rdi + %2]
  shr eax, %3
  mov dword [rdi + %1], eax
%endmacro

%macro Shr.miu64 3
  mov rax, qword [rdi + %2]
  shr rax, %3
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Or

%macro Or.mmb32 3
  mov eax, dword [rdi + %2]
  mov edx, dword [rdi + %3]
  or eax, edx
  mov dword [rdi + %1], eax
%endmacro

%macro Or.mmu8x32 3
  vmovdqu ymm0, yword [rdi + %2]
  vmovdqu ymm1, yword [rdi + %3]
  vpor ymm0, ymm0, ymm1
  vmovdqu yword [rdi + %1], ymm0
%endmacro

; -----------------------------------------------------------------------------
; Call

%macro Call 2
  push rdi
  mov rdi, [rdi + %2]
  call %1
  pop rdi
%endmacro

; -----------------------------------------------------------------------------
; Else

%macro Else 2
  jmp %2
  %1:
%endmacro

; -----------------------------------------------------------------------------
; New

extern malloc

%macro New 2
  push rdi
  push rdi
  mov rdi, %2_size
  call malloc
  pop rdi
  pop rdi
  mov [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Del

extern free

%macro Del.mstr 1
  push rdi
  push rdi
  mov rdi, [rdi + %1]
  call free
  pop rdi
  pop rdi
%endmacro

; -----------------------------------------------------------------------------
; Idw

%macro Idw.iiu16a 3
  mov rcx, %2
  mov rax, [rdi + %1]
  lea rax, [rax + rcx * 2]
  mov rcx, %3
  mov [rax], rcx
%endmacro

%macro Idw.mmu8x32a 3
  vmovdqu ymm0, yword [rdi + %3]
  mov rax, qword [rdi + %1]
  mov rcx, qword [rdi + %2]
  shl rcx, 5
  vmovdqu yword [rax + rcx], ymm0
%endmacro

%macro Idw.mru8x32a 3
  mov rax, qword [rdi + %1]
  mov rcx, qword [rdi + %2]
  shl rcx, 5
  vmovdqu yword [rax + rcx], %3
%endmacro

%macro Idw.mrru8x32a 3
  mov rax, qword [rdi + %1]
  mov rcx, %2
  shl rcx, 5
  vmovdqu yword [rax + rcx], %3
%endmacro

%macro Idw.rrru8x32a 3
  mov rcx, %2
  shl rcx, 5
  vmovdqu yword [%1 + rcx], %3
%endmacro

; -----------------------------------------------------------------------------
; Rdrand

%macro Rdrand.mu64 2
  rdrand rax
  mov qword [rdi + %1], rax
  setc byte [rdi + %2]
%endmacro

; -----------------------------------------------------------------------------
; Set

%macro Set.mu64 2
  mov al, byte [rdi + %2]
  cmp al, 0
  setne al
  movzx rax, al
  mov qword [rdi + %1], rax
%endmacro

%macro Set.ms64 2
  mov al, byte [rdi + %2]
  cmp al, 0
  setne al
  movzx rax, al
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Movzx

%macro Movzx.mu8 2
  mov al, byte [rdi + %2]
  movzx ax, al
  mov word [rdi + %1], ax
%endmacro

%macro Movzx.mu16 2
  mov ax, word [rdi + %2]
  movzx eax, ax
  mov dword [rdi + %1], eax
%endmacro

%macro Movzx.mu32 2
  mov eax, dword [rdi + %2]
  movzx rax, eax
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Usigned integer to larger unsigned types
; u8tou16, u16tou32, u32tou64

%macro u8tou16.m 2
  mov al, byte [rdi + %2]
  movzx ax, al
  mov word [rdi + %1], ax
%endmacro

%macro u16tou32.m 2
  mov ax, word [rdi + %2]
  movzx eax, ax
  mov dword [rdi + %1], eax
%endmacro

%macro u32tou64.m 2
  mov eax, dword [rdi + %2]
  movzx rax, eax
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Usigned integer to larger signed types
; u8tos16, u16tos32, u32tos64

%macro u8tos16.m 2
  mov al, byte [rdi + %2]
  movzx ax, al
  mov word [rdi + %1], ax
%endmacro

%macro u16tos32.m 2
  mov ax, word [rdi + %2]
  movzx eax, ax
  mov dword [rdi + %1], eax
%endmacro

%macro u32tos64.m 2
  mov eax, dword [rdi + %2]
  movzx rax, eax
  mov qword [rdi + %1], rax
%endmacro


; -----------------------------------------------------------------------------
; Signed integer to larger signed types
; s8tos16, s16tos32, s32tos64

%macro s8tos16.m 2
  mov al, byte [rdi + %2]
  movsx ax, al
  mov word [rdi + %1], ax
%endmacro

%macro s16tos32.m 2
  mov ax, word [rdi + %2]
  movsx eax, ax
  mov dword [rdi + %1], eax
%endmacro

%macro s32tos64.m 2
  mov eax, dword [rdi + %2]
  movsxd rax, eax
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Integer to floats
; u16tof32, s16tof32, u32tof64, s32tof64

%macro u16tof32.m 2
  mov ax, word [rdi + %2]
	movzx eax, ax
  cvtsi2ss xmm0, eax
  movss [rdi + %1], xmm0
%endmacro

%macro s16tof32.m 2
  mov ax, word [rdi + %2]
	movsx eax, ax
  cvtsi2ss xmm0, eax
  movss [rdi + %1], xmm0
%endmacro

%macro u32tof64.m 2
  mov eax, dword [rdi + %2]
	movzx rax, eax
  cvtsi2sd xmm0, rax
  movsd [rdi + %1], xmm0
%endmacro

%macro s32tof64.m 2
  mov eax, dword [rdi + %2]
  cvtsi2sd xmm0, eax
  movsd [rdi + %1], xmm0
%endmacro

; -----------------------------------------------------------------------------
; Floats to larger types
; f32tof64

%macro f32tof64.m 2
  movss xmm0, dword [rdi + %2]
  cvtss2sd xmm0, xmm0
  movsd [rdi + %1], xmm0
%endmacro

; -----------------------------------------------------------------------------
; Usigned integer to smaller unsigned types
; u16tou8, u32tou16, u64tou32

%macro u16tou8.m 2
  mov ax, word [rdi + %2]
  mov byte [rdi + %1], al
%endmacro

%macro u32tou16.m 2
  mov eax, dword [rdi + %2]
  mov word [rdi + %1], ax
%endmacro

%macro u64tou32.m 2
  mov rax, qword [rdi + %2]
  mov dword [rdi + %1], eax
%endmacro

; -----------------------------------------------------------------------------
; Usigned integer to smaller unsigned types
; s16tou8, s32tou16, s64tou32

%macro s16tou8.m 2
  mov ax, word [rdi + %2]
  mov byte [rdi + %1], al
%endmacro

%macro s32tou16.m 2
  mov eax, dword [rdi + %2]
  mov word [rdi + %1], ax
%endmacro

%macro s64tou32.m 2
  mov rax, qword [rdi + %2]
  mov dword [rdi + %1], eax
%endmacro


; -----------------------------------------------------------------------------
; Signed integer to smaller signed types
; s16tos8, s32tos16, s64tos32

%macro s16tos8.m 2
  mov ax, word [rdi + %2]
  mov byte [rdi + %1], al
%endmacro

%macro s32tos16.m 2
  mov eax, dword [rdi + %2]
  mov word [rdi + %1], ax
%endmacro

%macro s64tos32.m 2
  mov rax, qword [rdi + %2]
  mov dword [rdi + %1], eax
%endmacro

; -----------------------------------------------------------------------------
; floats to integers
; f32tou16, f32tos16, f64tou32, f64tos32

%macro f32tou16.m 2
  movss xmm0, [rdi + %2]
  cvttss2si eax, xmm0
  mov word [rdi + %1], ax
%endmacro

%macro f32tos16.m 2
  movss xmm0, [rdi + %2]
  cvttss2si eax, xmm0
  mov word [rdi + %1], ax
%endmacro

%macro f64tou32.m 2
  movsd xmm0, [rdi + %2]
  cvttsd2si eax, xmm0
  mov dword [rdi + %1], eax
%endmacro

%macro f64tos32.m 2
  movsd xmm0, [rdi + %2]
  cvttsd2si eax, xmm0
  mov dword [rdi + %1], eax
%endmacro

; -----------------------------------------------------------------------------
; Floats to smaller floats
; f64tof32

%macro f64tof32.m 2
  movsd xmm0, qword [rdi + %2]
  cvtsd2ss xmm0, xmm0
  movss dword [rdi + %1], xmm0
%endmacro

; -----------------------------------------------------------------------------
; Ror

%macro Ror.mib32 3
  mov eax, dword [rdi + %2]
  ror eax, %3
  mov dword [rdi + %1], eax
%endmacro

%macro Ror.mib64 3
  mov rax, qword [rdi + %2]
  ror rax, %3
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Rol

%macro Rol.mib32 3
  mov eax, dword [rdi + %2]
  rol eax, %3
  mov dword [rdi + %1], rax
%endmacro

%macro Rol.mib64 3
  mov rax, qword [rdi + %2]
  rol rax, %3
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Cmovcc

%macro Cmov.mms64 3
  mov rdx, qword [rdi + %2]
  mov al, byte [rdi + %3]
	test al, al
  cmovnz rax, rdx
  mov qword [rdi + %1], rax
%endmacro

%macro Cmoveq.mms64 4
  mov rax, qword [rdi + %3]
  mov rdx, qword [rdi + %4]
  mov rcx, qword [rdi + %2]
  cmp rax, rdx
  cmovz rax, rcx
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Xchg

%macro Xchg.ms64 2
  mov rax, qword [rdi + %2]
  xchg rax, qword [rdi + %1]
  mov qword [rdi + %2], rax
%endmacro

; -----------------------------------------------------------------------------
; Xadd

%macro Xadd.ms64 2
  mov rax, qword [rdi + %2]
  xadd qword [rdi + %1], rax
  mov qword [rdi + %2], rax
%endmacro

; -----------------------------------------------------------------------------
; Bswap

%macro Bswap.mb64 2
  mov rax, qword [rdi + %2]
  bswap rax
  mov qword [rdi + %1], rax
%endmacro

%macro Bswap.mb32 2
  mov eax, dword [rdi + %2]
  bswap eax
  mov dword [rdi + %1], eax
%endmacro

%macro Bswap.mb16 2
  mov ax, word [rdi + %2]
  xchg ah, al
  mov word [rdi + %1], ax
%endmacro

; -----------------------------------------------------------------------------
; Bsf -- Bit scan forward, find first set bit rightwards

%macro Bsf.mb64 2
  mov rax, qword [rdi + %2]
  bsf rax, rax
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Bsr -- Bit scan reverse, find first set bit leftwards

%macro Bsr.mb64 2
  mov rax, qword [rdi + %2]
  bsr rax, rax
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Bt -- Bit test

%macro Bt.miu64 3
  mov rax, qword [rdi + %2]
  bt rax, %3
	setc al
	movzx rax, al
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; CmpXchg -- Compare and Exchange

%macro CmpXchg.mms64 4
  mov rax, qword [rdi + %2]
  mov rcx, qword [rdi + %3]
  mov rdx, qword [rdi + %4]
  cmpxchg rcx, rdx
	setz r8b
  mov byte [rdi + %1], r8b
  mov qword [rdi + %2], rax
  mov qword [rdi + %3], rcx
%endmacro

; -----------------------------------------------------------------------------
; Crc32 -- Cyclic redundancy check

%macro Crc32.mu32 2
  mov eax, dword [rdi + %1]
  mov ecx, dword [rdi + %2]
  crc32 eax, ecx
  mov dword [rdi + %1], eax
%endmacro

; -----------------------------------------------------------------------------
; Movs -- Move String, Block copy

%macro Movs.mmu8a 3
  cld
  mov rsi, qword [rdi + %2]
  mov rcx, qword [rdi + %3]
  mov rbx, rdi
  mov rdi, qword [rdi + %1]
  rep movsb
  mov rdi, rbx
%endmacro

%macro Movs.mmu16a 3
  cld
  mov rsi, qword [rdi + %2]
  mov rcx, qword [rdi + %3]
  mov rbx, rdi
  mov rdi, qword [rdi + %1]
  rep movsw
  mov rdi, rbx
%endmacro

%macro Movs.mmu32a 3
  cld
  mov rsi, qword [rdi + %2]
  mov rcx, qword [rdi + %3]
  mov rbx, rdi
  mov rdi, qword [rdi + %1]
  rep movsd
  mov rdi, rbx
%endmacro

%macro Movs.mmu64a 3
  cld
  mov rsi, qword [rdi + %2]
  mov rcx, qword [rdi + %3]
  mov rbx, rdi
  mov rdi, qword [rdi + %1]
  rep movsq
  mov rdi, rbx
%endmacro

; -----------------------------------------------------------------------------
; Cmps -- Compare strings

%macro Cmps.mmu8a 4
  cld
  mov rsi, qword [rdi + %3]
  mov rcx, qword [rdi + %4]
  mov rbx, rdi
  mov rdi, qword [rdi + %2]
  rep cmpsb
  mov rdi, rbx
  setz byte [rdi + %1]
%endmacro

%macro Cmps.mmu16a 4
  cld
  mov rsi, qword [rdi + %3]
  mov rcx, qword [rdi + %4]
  mov rbx, rdi
  mov rdi, qword [rdi + %2]
  rep cmpsw
  mov rdi, rbx
  setz byte [rdi + %1]
%endmacro

%macro Cmps.mmu32a 4
  cld
  mov rsi, qword [rdi + %3]
  mov rcx, qword [rdi + %4]
  mov rbx, rdi
  mov rdi, qword [rdi + %2]
  rep cmpsd
  mov rdi, rbx
  setz byte [rdi + %1]
%endmacro

%macro Cmps.mmu64a 4
  cld
  mov rsi, qword [rdi + %3]
  mov rcx, qword [rdi + %4]
  mov rbx, rdi
  mov rdi, qword [rdi + %2]
  rep cmpsq
  mov rdi, rbx
  setz byte [rdi + %1]
%endmacro

; -----------------------------------------------------------------------------
; Scas -- Block scan, find index of byte %3 in array %2

%macro Scas.mmu8a 4
  cld
  mov al, byte [rdi + %3]
  mov rcx, qword [rdi + %4]
  mov rbx, rdi
  mov rdi, qword [rdi + %2]
  repne scasb
	sete r8b
	movzx r8, r8b
  mov rdi, rbx
	sub rcx, qword [rdi + %4]
  neg rcx
	sub rcx, r8
  mov qword [rdi + %1], rcx
%endmacro

; -----------------------------------------------------------------------------
; Popcnt -- Population count -- count ones set

%macro Popcnt.mb64 2
  mov rax, qword [rdi + %2]
	popcnt rax, rax
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Lzcnt -- Leading zero count

%macro Lzcnt.mb64 2
  mov rax, qword [rdi + %2]
	lzcnt rax, rax
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Tzcnt -- Trailing zero count

%macro Tzcnt.mb64 2
  mov rax, qword [rdi + %2]
	tzcnt rax, rax
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Rdtsc -- Read time stamp counter

%macro Rdtsc.mu64 1
	rdtsc
	shl rdx, 32
	or rax, rdx
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Rdpmc -- Read performance counter

%macro Rdpmc.mu64 2
	mov rcx, qword [rdi + %2]
	rdpmc
	shl rdx, 32
	or rax, rdx
  mov qword [rdi + %1], rax
%endmacro

; -----------------------------------------------------------------------------
; Nop -- No operation

%macro Nop 0
	nop
%endmacro

; -----------------------------------------------------------------------------
; Bextr -- Bitfield extract -- dest = bits a length b from src

%macro Bextr.mmb64 4
	mov rax, 0
	mov al, byte [rdi + %3]
	mov ah, byte [rdi + %4]
  mov rcx, qword [rdi + %2]
	bextr rdx, rcx, rax
  mov qword [rdi + %1], rdx
%endmacro

; -----------------------------------------------------------------------------
; Blsi -- Extract Lowest Set Isolated Bit

%macro Blsi.mb64 2
	mov rax, qword [rdi + %2]
	blsi rdx, rax
  mov qword [rdi + %1], rdx
%endmacro

; -----------------------------------------------------------------------------
; Blsmsk -- Generate bitmask

%macro Blsmsk.mb64 2
	mov rax, qword [rdi + %2]
	blsmsk rdx, rax
  mov qword [rdi + %1], rdx
%endmacro

; -----------------------------------------------------------------------------
; Blsr -- Clear lowest bit set

%macro Blsr.mb64 2
	mov rax, qword [rdi + %2]
	blsr rdx, rax
  mov qword [rdi + %1], rdx
%endmacro















